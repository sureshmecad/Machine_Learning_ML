{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat BOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.\n",
    "\n",
    "\n",
    "- **CatBoost** can automatically deal with **categorical variables and does not require extensive data preprocessing** like other machining learning algorithms.\n",
    "\n",
    "\n",
    "- **CatBoost** algorithm effectively deals with **categorical variables**. Thus you **should not perform one-hot-encoding** for categorical variables. Just **load the files, impute missing values**, and you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train, y_train, cat_features=([0,1,2,3,4,10]), eval_set=(x_test, y_test))\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Regression Problem\n",
    "from catboost import CatBoostRegressor\n",
    "model = CatBoostRegressor()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train, y_train, cat_features=([0,1,2,3,4,10]), eval_set=(x_test, y_test))\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "#### loss_function:\n",
    "- Defines the metric to be used for training\n",
    "\n",
    "#### iterations:\n",
    "- The maximum number of trees that can be built.\n",
    "\n",
    "\n",
    "- The final number of trees may be less than or equal to the number.\n",
    "\n",
    "#### learning_rate:\n",
    "- Defines the learnin rate.\n",
    "\n",
    "\n",
    "- Used for reducing the gradient step.\n",
    "\n",
    "#### border_count:\n",
    "- It specifies the number of splits for numerical features.\n",
    "\n",
    "\n",
    "- It is similar to the max_bin parameter.\n",
    "\n",
    "#### depth:\n",
    "- Defines the depth of the trees.\n",
    "\n",
    "#### random_seed:\n",
    "- This parameter is similar to the 'random_state' parameter we have seen previously.\n",
    "\n",
    "\n",
    "- It is an integer valus to define the random seed for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
